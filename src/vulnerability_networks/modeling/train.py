from typing import Optional

import lightning as L
import torch
import torch.nn.functional as F
from torch.utils.data import random_split
from torch_geometric.loader import DataLoader

from vulnerability_networks.algorithms.graph_neural_network import RankEdgeNet
from vulnerability_networks.config import PATH_MODELS, PATH_PROCESSED_DATA
from vulnerability_networks.dataset import NetworkDataset
from vulnerability_networks.losses import rank_loss_graph
from vulnerability_networks.metrics import kendall_tau_b_corrcoef, map_at_k, ndcg_at_k


class LightningRankEdgeNet(L.LightningModule):
    """Lightning RankEdgeNet"""

    def __init__(
        self,
        num_x_features: int,
        embedding_size: int,
        num_layers_msg: int,
        num_layers_mlp: int,
        msg_passing: str,
        loss_fn: str,
        dropout: float,
        edge_embedding_operator: str,
        loss_phi_fn: Optional[str] = None,
        lr=1e-3,
        **kwargs,
    ):
        super().__init__()
        assert msg_passing in ["GCN", "GAT"]
        if msg_passing == "GAT":
            assert isinstance(kwargs["v2"], bool)
            assert isinstance(kwargs["heads"], int)
        elif msg_passing == "GCN":
            assert kwargs.get("v2") is None
            assert kwargs.get("heads") is None
        assert edge_embedding_operator in ["hadamard", "mean", "concat"]
        assert loss_fn in ["MSE", "RANK"]
        if loss_phi_fn == "logistic":
            self.loss_phi_fn = lambda z: torch.log(1 + 1 / torch.exp(z))
        elif loss_phi_fn == "exponential":
            self.loss_phi_fn = lambda z: (1 / torch.exp(z))
        elif loss_phi_fn == "sqrt_exp":
            self.loss_phi_fn = lambda z: 1 - torch.sqrt(torch.exp(z) / (1 + torch.exp(z)))
        elif loss_fn == "RANK":
            raise Exception("Loss fn is rank but not a valid loss phi fn")
        else:
            self.loss_phi_fn = None
        self.lr = lr

        self.save_hyperparameters()

        # drop all the None values of the model
        kwargs = {k: v for k, v in kwargs.items() if v is not None}
        self.model = RankEdgeNet(
            num_x_features=num_x_features,
            embedding_size=embedding_size,
            num_layers_msg=num_layers_msg,
            num_layers_mlp=num_layers_mlp,
            msg_passing=msg_passing,
            dropout=dropout,
            edge_embedding_operator=edge_embedding_operator,
            **kwargs,
        )
        self.loss_fn = loss_fn

    def loss(self, *args, **kwargs):
        if self.loss_fn == "MSE":
            preds = F.sigmoid(kwargs["logits"])
            return F.mse_loss(preds, kwargs["targets"])
        elif self.loss_fn == "RANK":
            return rank_loss_graph(kwargs["logits"], kwargs["targets"], kwargs["graph_ids"], self.loss_phi_fn)

    def forward(self, batch):
        return self.model(batch.x, batch.edge_index, batch.edge_attr)

    def training_step(self, batch, batch_idx):
        edge_logits = self.model(batch.x, batch.edge_index, batch.edge_attr)
        train_loss = self.loss(logits=edge_logits, targets=batch.edge_y, graph_ids=batch.edge_attr_batch)
        # kendal_corr = kendall_tau_b_corrcoef(edge_logits, batch.edge_y, graph_ids=batch.edge_attr_batch)
        # map_at_k_perc = map_at_k(edge_logits, batch.edge_y, batch.edge_attr_batch)
        # ndcg_at_k_perc = ndcg_at_k(torch.sigmoid(edge_logits), batch.edge_y, batch.edge_attr_batch)
        self.log_dict(
            {
                f"train_loss_{self.loss_fn}": train_loss,
                # "train_kendall_corr": kendal_corr,
                # "train_map_at_k": map_at_k_perc,
                # "train_ndcg_at_k": ndcg_at_k_perc,
            },
            batch_size=batch.batch_size,
            on_epoch=True,
            on_step=False,
            prog_bar=True,
        )
        # self.log(f"train_loss_{self.loss_fn}", loss)
        return train_loss

    def validation_step(self, batch, batch_idx):
        edge_logits = self.model(batch.x, batch.edge_index, batch.edge_attr)
        val_loss = self.loss(logits=edge_logits, targets=batch.edge_y, graph_ids=batch.edge_attr_batch)
        kendal_corr = kendall_tau_b_corrcoef(edge_logits, batch.edge_y, graph_ids=batch.edge_attr_batch)
        map_at_k_perc = map_at_k(edge_logits, batch.edge_y, batch.edge_attr_batch)
        ndcg_at_k_perc = ndcg_at_k(torch.sigmoid(edge_logits), batch.edge_y, batch.edge_attr_batch)
        self.log_dict(
            {
                f"val_loss_{self.loss_fn}": val_loss,
                "val_kendall_corr_metric": kendal_corr,
                "val_map_at_k_metric": map_at_k_perc,
                "val_ndcg_at_k_metric": ndcg_at_k_perc,
            },
            batch_size=batch.batch_size,
            on_epoch=True,
            on_step=False,
            prog_bar=True,
        )
        return val_loss

    def test_step(self, batch, batch_idx):
        edge_logits = self.model(batch.x, batch.edge_index, batch.edge_attr)
        test_loss = self.loss(logits=edge_logits, targets=batch.edge_y, graph_ids=batch.edge_attr_batch)
        kendal_corr = kendall_tau_b_corrcoef(edge_logits, batch.edge_y, graph_ids=batch.edge_attr_batch)
        map_at_k_perc = map_at_k(edge_logits, batch.edge_y, batch.edge_attr_batch)
        ndcg_at_k_perc = ndcg_at_k(edge_logits, batch.edge_y, batch.edge_attr_batch)
        self.log_dict(
            {
                f"test_loss_{self.loss_fn}": test_loss,
                "test_kendall_corr_metric": kendal_corr,
                "test_map_at_k_metric": map_at_k_perc,
                "test_ndcg_at_k_metric": ndcg_at_k_perc,
            },
            batch_size=batch.batch_size,
        )
        return test_loss

    # def configure_optimizers(self):
    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
    #     return optimizer
    def configure_optimizers(self):
        opt = torch.optim.Adam(params=self.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt)
        lr_scheduler = {"scheduler": scheduler, "monitor": f"val_loss_{self.loss_fn}"}
        return {"optimizer": opt, "lr_scheduler": lr_scheduler}


class NetworkDataModule(L.LightningDataModule):
    def __init__(self, data_dir: str, batch_size: int):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size

    def setup(self, stage: Optional[str] = None) -> None:
        dataset = NetworkDataset(root=self.data_dir)
        dataset_size = len(dataset)
        train_size = int(0.8 * dataset_size)  # 70% for training
        val_size = int(0.1 * dataset_size)  # 15% for validation
        test_size = dataset_size - train_size - val_size  # 15% for testing
        self.dataset_train, self.dataset_val, self.dataset_test = random_split(
            dataset, [train_size, val_size, test_size], torch.Generator().manual_seed(42)
        )

    def train_dataloader(self) -> DataLoader:
        return DataLoader(
            self.dataset_train,
            batch_size=self.batch_size,
            shuffle=True,
            follow_batch=["edge_attr"],  # num_workers=10
        )

    def val_dataloader(self) -> DataLoader:
        return DataLoader(
            self.dataset_val,
            batch_size=self.batch_size,
            shuffle=False,
            follow_batch=["edge_attr"],  # num_workers=10
        )

    def test_dataloader(self) -> DataLoader:
        return DataLoader(
            self.dataset_test,
            batch_size=self.batch_size,
            shuffle=False,
            follow_batch=["edge_attr"],  # num_workers=10
        )


if __name__ == "__main__":
    ACCESSIBILITY_INDEX = "global_efficiency"
    MAX_EPOCHS = 100
    NUM_X_FEATURES = 3
    EMBEDDING_SIZE = 64
    NUM_LAYERS_MSG = 2
    NUM_LAYER_MLP = 3
    MSG_PASSING = "GAT"
    LOSS_FN = "RANK"
    DROPOUT = 0.25
    LOSS_PHI_FN = "sqrt_exp"
    EDGE_EMBEDDING_OPERATOR = "concat"
    LR = 1e-3
    BATCH_SIZE = 32
    GAT_HEADS = 8
    GAT_V2=True

    lit_model = LightningRankEdgeNet(
        num_x_features=NUM_X_FEATURES,
        embedding_size=EMBEDDING_SIZE,
        num_layers_msg=NUM_LAYERS_MSG,
        num_layers_mlp=NUM_LAYER_MLP,
        msg_passing=MSG_PASSING,
        loss_fn=LOSS_FN,
        dropout=DROPOUT,
        loss_phi_fn=LOSS_PHI_FN,
        edge_embedding_operator=EDGE_EMBEDDING_OPERATOR,
        lr=LR,
        heads=GAT_HEADS,
        v2=GAT_V2
    )

    dm = NetworkDataModule(PATH_PROCESSED_DATA / f"{ACCESSIBILITY_INDEX}_old", batch_size=16)

    tb_logger = L.pytorch.loggers.TensorBoardLogger(save_dir=PATH_MODELS, name=ACCESSIBILITY_INDEX)
    csv_logger = L.pytorch.loggers.CSVLogger(save_dir=PATH_MODELS, name=ACCESSIBILITY_INDEX, version=tb_logger.version)
    trainer = L.Trainer(
        max_epochs=MAX_EPOCHS,
        accelerator="gpu",
        logger=[tb_logger, csv_logger],
        callbacks=[L.pytorch.callbacks.EarlyStopping(monitor=f"val_loss_{LOSS_FN}", mode="min")],
        log_every_n_steps=5,
        deterministic=True,
    )
    trainer.fit(lit_model, dm)
    trainer.test(lit_model, dm)
